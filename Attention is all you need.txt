Initially RNN was used to generate the output for language conversion, and it used the word by word prediction method where if the word is correctlly predicted it give +ve weighted values whereas if incorrectlly predicted provide negative weighted values. This is RNN therefore used to work with both the Left to Right and Right to Left methods but when the sentence is long this method used to fail as the complex sentence mostlly have relation to words in the sentence and a single vector in case of RNN is not enough to provide the context for all the steps of conversion and RNN also by itself does not care much about context. To deal with this seq2seq method was introduced which need encoder and decoder and the decoder required to look up the entire sentence before predicting for a particular word which is made possible by attention layer. Earlier without attention the decoder used to look up the whole sentence each time it decodes a word and if the word is incorrectlly predicted it used to afftect the whole prediction, now the attention model helps the decoder to look at the entire sentence and selectively extract the information required to decode the input. Attention gives decoder access to all the encoder hidden states, the decoder still requires to predicts the next word and it cannot just look at the entire sentence everytime and here the attention model provide the weighting of the hidden states and then by looking at the weights the decoder model can choose which hidden layer to use for prediction. This was the basic idea behing attention model.